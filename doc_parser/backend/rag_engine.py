#rag_engine.py
from .config import DATA_DIR, OPENAI_API_KEY, EMBEDDING_MODEL, LLM_MODEL, BATCH_SIZE
from .chroma import ChromaManager
from openai import OpenAI
from pathlib import Path
import re
import openpyxl
import pandas as pd
import openpyxl
from .utils.parser import parse_competition_query
from .utils.search_excel import find_competition_ratio
from .utils.generate_graph import generate_base64_graph

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Chroma ë§¤ë‹ˆì € ì´ˆê¸°í™”
chroma = ChromaManager()

# ë¬¸ë‹¨ ìª¼ê°œê¸° ê´€ë ¨ ìƒìˆ˜
CHUNK_MAX_CHARS = 800
CHUNK_OVERLAP = 100


def split_into_chunks(text: str):
    """ë¬¸ì¥ì„ ì¼ì • ê¸¸ì´ë¡œ ë‚˜ëˆ”"""
    text = text.strip()
    if not text:
        return []
    if len(text) <= CHUNK_MAX_CHARS:
        return [text]

    chunks = []
    start = 0
    while start < len(text):
        end = start + CHUNK_MAX_CHARS
        chunk = text[start:end].strip()
        chunks.append(chunk)
        if end >= len(text):
            break
        start = end - CHUNK_OVERLAP
    return chunks


def _normalize_for_search(s: str) -> str:
    """ê²€ìƒ‰ìš©ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì •ê·œí™”: None ì•ˆì „ ì²˜ë¦¬, ì†Œë¬¸ìí™”, ëª¨ë“  ê³µë°± ì œê±°."""
    if s is None:
        return ""
    # strip and collapse unicode whitespace, then remove all whitespace for robust matching
    s = re.sub(r"\s+", "", str(s))
    return s.lower()


def index_all_documents():
    """output í´ë”ì˜ ëª¨ë“  .md ë° .xlsx ë¬¸ì„œë¥¼ ì½ê³  Chromaì— ì„ë² ë”© (ì¤‘ë³µ ë¬¸ì„œëŠ” ê±´ë„ˆëœ€)"""
    md_files = list(DATA_DIR.glob("*.md"))
    xlsx_files = list(DATA_DIR.glob("*.xlsx"))
    ids, texts, metadatas = [], [], []
    idx = 0

    # ì´ë¯¸ ì¸ë±ì‹±ëœ ë¬¸ì„œ ì´ë¦„ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    try:
        existing = chroma.collection.get(include=['metadatas'])
        existing_docs = set(meta['source'] for meta in existing['metadatas'] if meta and 'source' in meta)
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ì‹± ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        existing_docs = set()

    # ìƒˆ ë¬¸ì„œë§Œ ì¸ë±ì‹±
    all_files = md_files + xlsx_files
    new_docs = [f for f in all_files if f.name not in existing_docs]
    if not new_docs:
        print("ğŸ“š ìƒˆë¡œ ì¸ë±ì‹±í•  ë¬¸ì„œ ì—†ìŒ (ëª¨ë“  ë¬¸ì„œê°€ ì´ë¯¸ ë“±ë¡ë¨)")
        return

    for file in new_docs:
        print(f"â• ìƒˆ ë¬¸ì„œ ì¸ë±ì‹±: {file.name}")

        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        if file.suffix == ".md":
            content = file.read_text(encoding="utf-8")
        elif file.suffix == ".xlsx":
            try:
                import openpyxl
                wb = openpyxl.load_workbook(file, data_only=True, read_only=True)  # ìŠ¤íƒ€ì¼ ë¬´ì‹œ
                content_list = []

                for sheet_name in wb.sheetnames:
                    ws = wb[sheet_name]
                    rows = []
                    for row in ws.iter_rows(values_only=True):
                        row_str = " | ".join([str(cell) if cell is not None else "" for cell in row])
                        rows.append(row_str)
                    for i in range(0, len(rows), 5):
                        chunk_text = f"ì‹œíŠ¸ëª…: {sheet_name}\n" + "\n".join(rows[i:i+5])
                        content_list.append(chunk_text)
                content = "\n\n".join(content_list)
            except Exception as e:
                print(f"âš ï¸ {file.name} ì½ê¸° ì‹¤íŒ¨: {e}")
                continue

        else:
            continue

        # ë¬¸ë‹¨ ë‹¨ìœ„ ë¶„ë¦¬ ë° ì²­í¬ ë¶„í• 
        parts = [seg.strip() for seg in re.split(r'\n{2,}', content) if seg.strip()]
        for seg in parts:
            chunks = split_into_chunks(seg)
            for ci, chunk in enumerate(chunks, start=1):
                idx += 1
                ids.append(f"{file.name}__{idx}")
                texts.append(chunk)
                metadatas.append({"source": file.name, "chunk_idx": ci})

    if not texts:
        print("âš ï¸ ì¸ë±ì‹±í•  ë¬¸ë‹¨ ì—†ìŒ")
        return

    print("í˜„ì¬ ì €ì¥ëœ ë¬¸ë‹¨ ìˆ˜:", chroma.count())

    # âœ… ì„ë² ë”© ìˆ˜í–‰ ë° ë°°ì¹˜ ë‹¨ìœ„ë¡œ Chroma ì¶”ê°€
    embeddings = []
    for i in range(0, len(texts), BATCH_SIZE):
        batch_texts = texts[i:i + BATCH_SIZE]
        resp = openai_client.embeddings.create(model=EMBEDDING_MODEL, input=batch_texts)
        embeddings.extend([item.embedding for item in resp.data])

    MAX_CHROMA_BATCH = 5000
    for i in range(0, len(ids), MAX_CHROMA_BATCH):
        batch_ids = ids[i:i+MAX_CHROMA_BATCH]
        batch_texts = texts[i:i+MAX_CHROMA_BATCH]
        batch_embeds = embeddings[i:i+MAX_CHROMA_BATCH]
        batch_metas = metadatas[i:i+MAX_CHROMA_BATCH]

        chroma.add_documents(batch_ids, batch_texts, batch_embeds, batch_metas)
        print(f"ğŸ§© Chromaì— {len(batch_ids)}ê°œ ë¬¸ë‹¨ ì¶”ê°€ ì™„ë£Œ ({i + len(batch_ids)}/{len(ids)})")

    chroma.persist()
    print(f"âœ… ìƒˆë¡œ ì¸ë±ì‹±ëœ ë¬¸ë‹¨ ìˆ˜: {len(ids)}ê°œ")


def query_and_answer(query: str, top_k=60):
    # ë¨¼ì € competition_handlerë¡œ ìˆ˜ì¹˜ í™•ì¸
    comp_res = competition_handler(query)
    if comp_res:
        if comp_res["type"] == "text":
            text_values = comp_res["content"]
            answer_lines = []
            for year, val in text_values.items():
                if val is None:
                    answer_lines.append(f"{year}ë…„: í•´ë‹¹ ì—°ë„ì˜ ê²½ìŸë¥  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # âœ… ìˆ«ìë“  ë¬¸ìì—´ì´ë“  ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
                    answer_lines.append(f"{year}ë…„: {str(val)}")
            answer = "\n".join(answer_lines)
            # âœ… ìµœì¢… return ë³´ì¥
            return {
                "type": "text",
                "answer": answer,
                "sources": []
            }
        else:
            return {
                "type": "graph",
                "answer": "ê·¸ë˜í”„ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "sources": []
            }

    # RAG ê²€ìƒ‰ (ìˆ˜ì¹˜ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
    q_emb_resp = openai_client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=[query]
    )
    q_emb = q_emb_resp.data[0].embedding
    res = chroma.query(q_emb, top_k)
    docs = res.get("documents", [[]])[0]
    distances = res.get("distances", [[]])[0]

    RAG_THRESHOLD = 1.4
    if not docs or (len(distances) > 0 and distances[0] > RAG_THRESHOLD):
        # ë¬¸ì„œì™€ ë¬´ê´€ â†’ ì¼ë°˜ LLM ë°˜ì‘
        general_prompt = f"ë‹¤ìŒ ì‚¬ìš©ì ë©”ì‹œì§€ì— ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.\nì‚¬ìš©ì ë©”ì‹œì§€: {query}\n"
        resp = openai_client.responses.create(
            model=LLM_MODEL,
            input=general_prompt,
            max_output_tokens=256
        )
        return getattr(resp, "output_text", str(resp))

    # ë¬¸ì„œ ê¸°ë°˜ RAG ë‹µë³€ (ì„¤ëª…ìš©)
    prompt = f"""
    ë„ˆëŠ” 'ì•ˆì–‘ëŒ€í•™êµ ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ ì•ˆë‚´ ì±—ë´‡'ì´ë‹¤.
    ì•„ë˜ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©í•´ ë‹µí•´ë¼.

    ì‘ì„± ê·œì¹™(ì¤‘ìš”):
    - '1. 2. 3.' ê°™ì€ ë²ˆí˜¸ ë‚˜ì—´ ê¸ˆì§€
    - í‘œë¥¼ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ì§€ ë§ê³ , ì‚¬ëŒì´ ë§í•˜ë“¯ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½
    - ë¨¼ì € 1~2ë¬¸ì¥ìœ¼ë¡œ ì „ì²´ ìš”ì•½ â†’ ê·¸ ë‹¤ìŒ í•µì‹¬ë§Œ 3~6ì¤„ë¡œ ì •ë¦¬
    - ì§ˆë¬¸ì´ "ë­ ìˆì–´?/ë­ì•¼?/ì•Œë ¤ì¤˜" ê°™ì€ íƒìƒ‰í˜•ì´ë©´, ì„±ê²©/ì¹´í…Œê³ ë¦¬ë¡œ ë¬¶ì–´ì„œ ì„¤ëª…
    - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•´ë¼
    - ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, ê³¼í•˜ê²Œ ê¸¸ì§€ ì•Šê²Œ

    === ë¬¸ì„œ ë‚´ìš© ===
    {chr(10).join(docs)}

    === ì‚¬ìš©ì ì§ˆë¬¸ ===
    {query}

    === ë‹µë³€ ===
    """
    resp = openai_client.responses.create(
        model=LLM_MODEL,
        input=prompt,
        max_output_tokens=512
    )
    answer = getattr(resp, "output_text", str(resp))

    print("ğŸ” ê²€ìƒ‰ ê²°ê³¼ ê±°ë¦¬:", distances)
    if docs:
        print("ğŸ” ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì„œ:", docs[0][:200], "...")
    print("ğŸ” ì¿¼ë¦¬ ì„ë² ë”© dimension:", len(q_emb))

    return answer


ADMISSION_ALIAS = {
    "ê¸°íšŒê· í˜•ì „í˜•": "ê¸°íšŒê· í˜•ì „í˜•",
    "ê³ ë¥¸ê¸°íšŒì „í˜•": "ê³ ë¥¸ê¸°íšŒì „í˜•",
    "ì¼ë°˜ì „í˜•": "ì¼ë°˜ì „í˜•",
    "ì •ì‹œ": "ì •ì‹œ",
}


def competition_handler(query: str):
    parsed = parse_competition_query(query)

    print("ğŸ”¹ parsed query:", parsed)

    # í•„ìˆ˜ê°’ì´ ì—†ìœ¼ë©´ ê²½ìŸë¥  ì²˜ë¦¬ ëŒ€ìƒ ì•„ë‹˜
    if not parsed.get("years") or not parsed.get("university") or not parsed.get("major"):
        return None

    results = {}

    # ì •ê·œí™”ëœ ê²€ìƒ‰ ë¬¸ìì—´ ë§Œë“¤ê¸°
    # major, university, admission ë“±ì„ ê²€ìƒ‰ìš©ìœ¼ë¡œ ì •ê·œí™”
    raw_univ = parsed.get("university")
    raw_major = parsed.get("major")
    raw_admission = parsed.get("admission")

    norm_univ = _normalize_for_search(raw_univ)
    # í•™ê³¼ëª…ì€ ë‚´ë¶€ ê³µë°±ì´ ì„ì—¬ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ ëª¨ë“  ê³µë°± ì œê±°
    norm_major = _normalize_for_search(raw_major)
    # ì „í˜•ì€ Noneì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    norm_admission = _normalize_for_search(raw_admission)

    # ì „í˜• alias ì ìš© (alias í‚¤ë„ ì •ê·œí™”í•´ì„œ ë§¤ì¹­)
    # ADMISSION_ALIASì˜ í‚¤ê°€ í•œê¸€ ê³µë°± í¬í•¨ ìƒíƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ normalizeí•´ì„œ í™•ì¸
    alias_map = { _normalize_for_search(k): v for k, v in ADMISSION_ALIAS.items() }
    if norm_admission in alias_map:
        norm_admission = alias_map[norm_admission]

    for y in parsed.get("years", []):
        # find_competition_ratioì— ì „ë‹¬í•  ë•ŒëŠ” ì´ë¯¸ ì •ê·œí™”ëœ ê°’ì„ ì‚¬ìš©
        try:
            val = find_competition_ratio(
                y,
                norm_univ,
                norm_major,
                norm_admission
            )
        except Exception as e:
            print(f"âš ï¸ find_competition_ratio í˜¸ì¶œ ì¤‘ ì˜ˆì™¸: {e}")
            val = None

        # ìˆ«ì í¬ë§· í†µì¼
        if val is not None:
            try:
                val = round(float(val), 2)
            except Exception:
                # ìˆ«ìë¡œ ë³€í™˜ ë¶ˆê°€í•˜ë©´ ì›ë˜ ê°’ ê·¸ëŒ€ë¡œ ë‘ê¸°
                pass

        results[y] = val

        print(f"ğŸ”¹ year={y}, admission={norm_admission}, found val={val}")

    years = parsed.get("years", [])
    force_graph = len(years) >= 2

    # ê·¸ë˜í”„ ìš”êµ¬ ì—¬ë¶€(ë˜ëŠ” ê°•ì œ ê·¸ë˜í”„)
    if not parsed.get("wants_graph") and not force_graph:
        return {
            "type": "text",
            "content": results
        }

    img_b64 = generate_base64_graph(results)
    return {
        "type": "graph",
        "content": img_b64,
        "values": results
    }


# ì§ì ‘ ì‹¤í–‰ ì‹œ ë¬¸ì„œ ì¸ë±ì‹±
if __name__ == "__main__":
    print("ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘...")
    index_all_documents()
    print("ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")
